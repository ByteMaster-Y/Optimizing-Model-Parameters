{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrhGvGjA8mp-",
        "outputId": "da34fbec-2be7-4edc-8122-d7de5e7fe3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:00<00:00, 114MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 5.98MB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 61.0MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 19.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# 미니 배치는 기울기(gradient)를 효율적으로 계산하고 업데이트하기 위해 전체 데이터셋에서 추출하는 작은 데이터 묶음.\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits # 활성화 함수(activation function)를 거치기 전의 결과 값\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "2ok6B9_o91Rt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "NrrG2KE-94q4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저를 쓰는 이유는 모델의 손실을 최소화하는 최적의 가중치(parameters)를 효율적으로 찾아내기 위함.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#  옵티마이저가 찾아낸 가중치(parameters)를 가지고 경사하강법을 통해 손실 함수 값을 줄이는 방향으로 가중치를 업데이트합니다."
      ],
      "metadata": {
        "id": "-_c-xOpd96x0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset) # 전체 데이터셋의 총 샘플 수를 구하는 역할\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X) # batch * batch_size + len(X) -> 직전 배치까지 처리한 샘플의 총 개수\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad(): # 모델을 평가할 때는 가중치를 바꿀 필요가 없습니다. 따라서 기울기 계산을 생략함으로써 메모리를 절약하고 연산 속도를 크게 높일 수 있습니다.\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "IfTFBpH9-jOd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pred.argmax(1): pred 텐서는 모델이 각 클래스에 대해 예측한 원시 점수(로짓)를 담고 있습니다. 예를 들어, 3개의 클래스 분류 문제에서 한 샘플에 대한 예측값이 [0.2, 1.5, -0.1]라면, argmax(1)은 가장 큰 값인 1.5의 인덱스인 1을 반환합니다. 이 결과는 모델이 예측한 클래스를 의미합니다.\n",
        "\n",
        "== y: pred.argmax(1)이 반환한 예측 클래스(인덱스)와 실제 정답 레이블인 y를 비교합니다. 이 비교 결과는 True 또는 False로 이루어진 불리언 텐서가 됩니다. 예측이 맞으면 True, 틀리면 False가 되는 식입니다."
      ],
      "metadata": {
        "id": "iLuClWfYAM5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKtwd4ia_3Gx",
        "outputId": "17a22eb3-5e75-4cad-b89e-49626caf27d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298171  [   64/60000]\n",
            "loss: 2.291299  [ 6464/60000]\n",
            "loss: 2.271887  [12864/60000]\n",
            "loss: 2.264206  [19264/60000]\n",
            "loss: 2.240529  [25664/60000]\n",
            "loss: 2.225499  [32064/60000]\n",
            "loss: 2.225888  [38464/60000]\n",
            "loss: 2.196253  [44864/60000]\n",
            "loss: 2.187274  [51264/60000]\n",
            "loss: 2.166602  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.5%, Avg loss: 2.158272 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.167623  [   64/60000]\n",
            "loss: 2.155992  [ 6464/60000]\n",
            "loss: 2.100586  [12864/60000]\n",
            "loss: 2.112429  [19264/60000]\n",
            "loss: 2.053012  [25664/60000]\n",
            "loss: 2.015520  [32064/60000]\n",
            "loss: 2.030861  [38464/60000]\n",
            "loss: 1.958388  [44864/60000]\n",
            "loss: 1.956955  [51264/60000]\n",
            "loss: 1.898381  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 1.890497 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.918789  [   64/60000]\n",
            "loss: 1.888941  [ 6464/60000]\n",
            "loss: 1.774575  [12864/60000]\n",
            "loss: 1.813228  [19264/60000]\n",
            "loss: 1.701374  [25664/60000]\n",
            "loss: 1.671389  [32064/60000]\n",
            "loss: 1.684320  [38464/60000]\n",
            "loss: 1.597056  [44864/60000]\n",
            "loss: 1.613871  [51264/60000]\n",
            "loss: 1.520354  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.8%, Avg loss: 1.534076 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.594603  [   64/60000]\n",
            "loss: 1.564004  [ 6464/60000]\n",
            "loss: 1.420851  [12864/60000]\n",
            "loss: 1.486194  [19264/60000]\n",
            "loss: 1.368298  [25664/60000]\n",
            "loss: 1.374497  [32064/60000]\n",
            "loss: 1.379608  [38464/60000]\n",
            "loss: 1.318501  [44864/60000]\n",
            "loss: 1.342122  [51264/60000]\n",
            "loss: 1.248478  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.272559 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.345094  [   64/60000]\n",
            "loss: 1.329795  [ 6464/60000]\n",
            "loss: 1.171553  [12864/60000]\n",
            "loss: 1.267022  [19264/60000]\n",
            "loss: 1.142276  [25664/60000]\n",
            "loss: 1.173867  [32064/60000]\n",
            "loss: 1.186795  [38464/60000]\n",
            "loss: 1.138839  [44864/60000]\n",
            "loss: 1.167296  [51264/60000]\n",
            "loss: 1.086031  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.5%, Avg loss: 1.104927 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.173590  [   64/60000]\n",
            "loss: 1.177422  [ 6464/60000]\n",
            "loss: 1.001751  [12864/60000]\n",
            "loss: 1.126477  [19264/60000]\n",
            "loss: 0.996859  [25664/60000]\n",
            "loss: 1.035532  [32064/60000]\n",
            "loss: 1.064912  [38464/60000]\n",
            "loss: 1.021412  [44864/60000]\n",
            "loss: 1.050765  [51264/60000]\n",
            "loss: 0.982814  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.7%, Avg loss: 0.994527 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.051335  [   64/60000]\n",
            "loss: 1.077634  [ 6464/60000]\n",
            "loss: 0.883267  [12864/60000]\n",
            "loss: 1.031920  [19264/60000]\n",
            "loss: 0.903227  [25664/60000]\n",
            "loss: 0.938083  [32064/60000]\n",
            "loss: 0.984654  [38464/60000]\n",
            "loss: 0.943964  [44864/60000]\n",
            "loss: 0.969488  [51264/60000]\n",
            "loss: 0.913232  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.8%, Avg loss: 0.919081 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.960987  [   64/60000]\n",
            "loss: 1.008405  [ 6464/60000]\n",
            "loss: 0.798621  [12864/60000]\n",
            "loss: 0.965741  [19264/60000]\n",
            "loss: 0.840482  [25664/60000]\n",
            "loss: 0.867663  [32064/60000]\n",
            "loss: 0.928558  [38464/60000]\n",
            "loss: 0.892297  [44864/60000]\n",
            "loss: 0.911063  [51264/60000]\n",
            "loss: 0.863273  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.865251 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.892028  [   64/60000]\n",
            "loss: 0.957282  [ 6464/60000]\n",
            "loss: 0.735732  [12864/60000]\n",
            "loss: 0.916997  [19264/60000]\n",
            "loss: 0.795868  [25664/60000]\n",
            "loss: 0.815337  [32064/60000]\n",
            "loss: 0.886762  [38464/60000]\n",
            "loss: 0.856515  [44864/60000]\n",
            "loss: 0.867796  [51264/60000]\n",
            "loss: 0.825026  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.824849 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.837236  [   64/60000]\n",
            "loss: 0.916588  [ 6464/60000]\n",
            "loss: 0.686909  [12864/60000]\n",
            "loss: 0.879386  [19264/60000]\n",
            "loss: 0.762214  [25664/60000]\n",
            "loss: 0.775639  [32064/60000]\n",
            "loss: 0.853341  [38464/60000]\n",
            "loss: 0.830404  [44864/60000]\n",
            "loss: 0.834527  [51264/60000]\n",
            "loss: 0.794246  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 0.792939 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}
